{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\alice\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\alice\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import string\n",
    "import pandas_profiling\n",
    "\n",
    "import datetime\n",
    "from textblob import TextBlob #for polarity and sentiment analysis\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize  \n",
    "from nltk.corpus import stopwords\n",
    "import swifter\n",
    "from pprint import pprint\n",
    "\n",
    "import gensim\n",
    "from gensim.summarization import summarize, keywords\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "nltk.download('wordnet')\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Related to Time Features\n",
    "def trunc_at(s, d,n):\n",
    "    \"Returns string truncated at the n'th occurrence of the delimiter, d.\"\n",
    "    return d.join(s.split(d, n)[:n])\n",
    "def convert_date(date):\n",
    "    '''First uses trunc_at to get rid of time of day the article was posted. Then it turns the remaining date into\n",
    "       integers from 0-6 where 0 represents Sunday.\n",
    "    '''\n",
    "    strip_date = trunc_at(date,\",\",2)\n",
    "    dummy_date = datetime.datetime.strptime(strip_date, '%b %d, %Y').strftime('%w')\n",
    "    return int(dummy_date)\n",
    "\n",
    "#Related to Weekend Dummy Variable Analysis\n",
    "def weekend_or_not(day):\n",
    "    '''Returns whether or not an article was published on a weekend or weekday. Day is an argument passed in\n",
    "       with integers 0-6 where 0 represents Sunday.\n",
    "    '''\n",
    "    #0 represents sunday and 6 represents saturday\n",
    "    if day not in (0,6):\n",
    "        return \"weekday\"\n",
    "    else:\n",
    "        return \"weekend\"\n",
    "    \n",
    "#Related to Month Dummy Variable Analysis\n",
    "def new_convert_date(date):\n",
    "    '''First uses trunc_at to get rid of time of day the article was posted. Then it turns the remaining date into\n",
    "       integers from 0-6 where 0 represents Sunday.\n",
    "    '''\n",
    "    strip_date = trunc_at(date,\",\",2)\n",
    "    dummy_date = datetime.datetime.strptime(strip_date, '%b %d, %Y').strftime('%b')\n",
    "    return dummy_date\n",
    "\n",
    "#Related to Polarity Analysis\n",
    "def find_polarity(words):\n",
    "    '''Returns the polarity of every word in an article where the argument words are the words in an article.\n",
    "    '''\n",
    "    polarity_words = [TextBlob(word).sentiment.polarity for word in words]\n",
    "    return polarity_words\n",
    "def find_pos_words(polarity_words):\n",
    "    '''Returns the polarities of all the positive words in an article with the polarity of all words in an \n",
    "       article as function argument. Postive word is defined as a word with a polarity greater than 0.\n",
    "    '''\n",
    "    pos_words = [word for word in polarity_words if word > 0] \n",
    "    return pos_words\n",
    "def find_neg_words(polarity_words):\n",
    "    '''Returns the polarities of all the negative words in an article with the polarity of all words in an \n",
    "       article as function argument. Negative word is defined as a word with a polarity less than 0.\n",
    "    '''\n",
    "    neg_words = [word for word in polarity_words if word < 0]\n",
    "    return neg_words\n",
    "def num_neu_words(polarity_words):\n",
    "    '''Returns the number of neutral words in an article with the polarity of all word in an article as function\n",
    "       argument. Neutral word id defined as a word with a polarity equal to 0.\n",
    "    '''\n",
    "    neu_words = [word for word in polarity_words if word == 0]\n",
    "    neu_num = len(neu_words)\n",
    "    return neu_num\n",
    "\n",
    "\n",
    "\n",
    "#Related to Uninque Words Analysis\n",
    "stop_words = stopwords.words(\"english\")\n",
    "def tokenize(text):\n",
    "    '''Returns the tokenized words in an article.\n",
    "    '''\n",
    "    tokenize_words = word_tokenize(text)\n",
    "    tokens = [word for word in tokenize_words if word not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "\n",
    "# Related to Unique Words Analysis\n",
    "def preprocess_words(text):\n",
    "    '''Returns words lowercased and without punctuation.\n",
    "    '''\n",
    "    words = text.split()\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    stripped_words = [w.translate(table) for w in words]\n",
    "    words = [word.lower() for word in stripped_words]\n",
    "    return words\n",
    "def non_stop_words(text):\n",
    "    '''Returns non_stop_words in text after applying the preprocess_words function to lowercase the words and \n",
    "       remove punctuation.\n",
    "    '''\n",
    "    words = preprocess_words(text)\n",
    "    non_stop_words = [word for word in words if word not in stop_words]\n",
    "    return non_stop_words\n",
    "\n",
    "# Related to LDA Analysis \n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "def lemmatize_stemming(text):\n",
    "    '''Lemmatize and stem words. Lemmatizing changes third person words to first person and verbs in past and \n",
    "       future tenses to present tenses. Stemming reduces words to their root form.\n",
    "    '''\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    '''Preprocesses text by lowercasing and tokenizing words through gensim.utils.simple_preprocess and \n",
    "       removing stop words and only keeping words greater than three characters.\n",
    "       Then applies the lemmatize_stemming function to lemmatize and stem words.\n",
    "    '''\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result\n",
    "\n",
    "def find_probs(prob_list,num):\n",
    "    '''Takes in a 2d list with the probabilities for each topic per document with index for topic. Returns \n",
    "       just a list of probabilities in order of topics per document.\n",
    "       num parameter = number of topics for lda model.\n",
    "    '''\n",
    "    probs =[prob_list[i][1] for i in range(num)]\n",
    "    return probs\n",
    "\n",
    "\n",
    "#Related to TimeDelta Analysis\n",
    "def timedelta(time,run_date):\n",
    "    strip_date = trunc_at(time,\",\",2)\n",
    "    date = datetime.datetime.strptime(strip_date, '%b %d, %Y')\n",
    "    timedelta_days = (run_date - date).days\n",
    "    return timedelta_days\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"final_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['Unnamed: 0'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "time = df[\"time\"] #time of publication for every article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Timedelta Feature \n",
    "time_run = 'Dec 1, 2020'\n",
    "run_date = datetime.datetime.strptime(time_run,'%b %d, %Y')\n",
    "df[\"timedelta\"] = time.apply(lambda x: timedelta(x,run_date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = df[\"text\"] \n",
    "df['num_keywords'] = [len(keywords(article)) for article in text] #number of keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "for i in df.columns:\n",
    "    print(df[i].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(how='any', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7446"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"final_7k.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['link', 'title', 'text', 'view', 'topic', 'time', 'kw_min_min',\n",
       "       'kw_min_max', 'kw_min_avg', 'kw_max_min', 'kw_max_max', 'kw_max_avg',\n",
       "       'kw_avg_min', 'kw_avg_max', 'kw_avg_avg', 'Business', 'Innovation',\n",
       "       'Leadership', 'Lifestyle', 'Money', 'month', 'Apr', 'Aug', 'Dec', 'Feb',\n",
       "       'Jan', 'Jul', 'Jun', 'Mar', 'May', 'Nov', 'Oct', 'Sep',\n",
       "       'n_tokens_title:', 'n_tokens_content', 'n_unique_tokens',\n",
       "       'average_token_length', 'n_non_stop_words', 'n_non_stop_unique_tokens',\n",
       "       'day_of_week', 'friday', 'monday', 'saturday', 'sunday', 'thursday',\n",
       "       'tuesday', 'wednesday', 'weekend_or_weekday', 'weekday', 'weekend',\n",
       "       'global_sentiment_polarity', 'global_subjectivity',\n",
       "       'title_sentiment_polarity', 'abs_title_sentiment_polarity',\n",
       "       'title_subjectivity', 'abs_title_subjectivity',\n",
       "       'global_rate_positive_words', 'global_rate_negative_words',\n",
       "       'rate_positive_words', 'rate_negative_words', 'avg_positive_polarity',\n",
       "       'min_positive_polarity', 'max_positive_polarity',\n",
       "       'avg_negative_polarity', 'min_negative_polarity',\n",
       "       'max_negative_polarity', 'timedelta', 'LDA_00', 'LDA_01', 'LDA_02',\n",
       "       'LDA_03', 'LDA_04', 'num_keywords'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Summarize dataset:  92%|███████████████████████████████████████▎   | 76/83 [00:24<00:07,  1.03s/it, Get scatter matrix]"
     ]
    }
   ],
   "source": [
    "df.replace('', np.nan, inplace=True)\n",
    "\n",
    "# select features for summary stats\n",
    "profile = pandas_profiling.ProfileReport(df[['link', 'title', 'text', 'view', 'Business', 'Innovation', 'Leadership',\n",
    "       'Lifestyle', 'Money', 'Apr', 'Aug', 'Dec', 'Feb', 'Jan', 'Jul', 'Jun',\n",
    "       'Mar', 'May', 'Nov', 'Oct', 'Sep', 'n_tokens_title:',\n",
    "       'n_tokens_content', 'n_unique_tokens', 'average_token_length',\n",
    "       'n_non_stop_words', 'n_non_stop_unique_tokens', 'friday', 'monday',\n",
    "       'saturday', 'sunday', 'thursday', 'tuesday', 'wednesday', 'weekday',\n",
    "       'weekend', 'global_sentiment_polarity', 'global_subjectivity',\n",
    "       'title_sentiment_polarity', 'abs_title_sentiment_polarity',\n",
    "       'title_subjectivity', 'abs_title_subjectivity',\n",
    "       'global_rate_positive_words', 'global_rate_negative_words',\n",
    "       'rate_positive_words', 'rate_negative_words', 'avg_positive_polarity',\n",
    "       'min_positive_polarity', 'max_positive_polarity',\n",
    "       'avg_negative_polarity', 'min_negative_polarity',\n",
    "       'max_negative_polarity', 'LDA_00', 'LDA_01', 'LDA_02', 'LDA_03',\n",
    "       'LDA_04', 'timedelta', 'kw_min_min', 'kw_min_max', 'kw_min_avg',\n",
    "       'kw_max_min', 'kw_max_max', 'kw_max_avg', 'kw_avg_min', 'kw_avg_max',\n",
    "       'kw_avg_avg', 'num_keywords']]) \n",
    "profile.to_notebook_iframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('base': conda)",
   "language": "python",
   "name": "python37664bitbaseconda4c633f59a46f40f2b95e5029fe3d1c66"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
